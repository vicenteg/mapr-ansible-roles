
--- 
- hosts: cluster
  gather_facts: no
  max_fail_percentage: 0
  tasks:
    - name: get ec2 facts
      ec2_facts:
      register: ec2_facts

- hosts: cluster
  tasks:
    - group_by: key={{ansible_virtualization_type}}
    - group_by: key={{ansible_distribution}}

# RedHat derived distro specific prereqs
- hosts: CentOS;RedHat
  max_fail_percentage: 0
  sudo: yes
  roles:
    - { role: mapr-redhat-prereqs,
        mapr_user_pw: '$1$yoPLWBQ6$6fvQchDTBu3Ccs3PVURpA.',
        mapr_uid: 2147483632,
        mapr_gid: 2147483632,
        mapr_home: '/home/mapr',
        mapr_version: 'v3.1.1' } # will this work everywhere it's needed?

# Debian derived distro specific prereqs
- hosts: Debian;Ubuntu
  max_fail_percentage: 0
  sudo: yes
  roles:
    - { role: mapr-debian-prereqs,
        mapr_user_pw: '$1$yoPLWBQ6$6fvQchDTBu3Ccs3PVURpA.',
        mapr_uid: 2147483632,
        mapr_gid: 2147483632,
        mapr_home: '/home/mapr',
        mapr_version: 'v3.1.1' } # will this work everywhere it's needed?

- hosts: zookeepers
  max_fail_percentage: 0
  sudo: yes
  roles:
    - zookeeper

- hosts: cldb
  max_fail_percentage: 0
  sudo: yes
  roles:
    - mapr-cldb

- hosts: jobtracker
  max_fail_percentage: 0
  sudo: yes
  roles:
    - mapr-jobtracker

- hosts: tasktracker
  sudo: yes
  roles:
    - mapr-tasktracker

- hosts: webserver
  sudo: yes
  roles:
    - webserver

- hosts: nfs
  sudo: yes
  roles:
    - mapr-nfs

- hosts: fileserver
  sudo: yes
  roles:
    - { role: mapr-fileserver,
        mapr_disks: [ '/dev/xvdf', '/dev/xvdg' ],
        java_home: '/usr',
        cluster_name: '{{mapr_cluster_name}}' } # will this work everywhere it's needed?

- hosts: zookeepers
  sudo: yes
  tasks:
    - name: start zookeeper on ZK nodes
      service: name=mapr-zookeeper state=started enabled=yes
      register: zk_started

    - name: wait for zookeeper to be listening
      wait_for: port=5181 delay=10 timeout=90

    - name: wait for a few seconds to let zookeepers initialize
      pause: seconds=15
      when: zk_started.changed

    - name: get zookeeper status
      command: service mapr-zookeeper qstatus
      register: zk_qstatus

- hosts: cldb
  sudo: yes
  tasks:
    - name: start warden on CLDBs first per doc.mapr.com
      service: name=mapr-warden state=started enabled=yes
      register: cldb_started

    - name: pause for a bit and wait for CLDBs to come up
      pause: seconds=15
      when: cldb_started.changed

- hosts: cluster
  sudo: yes
  tasks:
    - name: start the warden on all nodes
      service: name={{item}} state=started
      with_items:
        - mapr-warden
      register: warden_started

    - name: wait a bit before proceeding
      pause: seconds=10
      when: warden_started.changed

# Configure all cluster nodes with public key access
# This allows passwordless SSH between cluster nodes for root and mapr users.
- include: authorized_keys.yml

#
# Having installed and started the cluster, install optional ecosystem and client services.
# Some of the below will need to be uncommented, or be run manually.
#

# uncomment the following line to install mysql (hive metastore and metrics)
- include: install_mysql.yml

# uncomment the following line to install spark (spark, shark, spark-master)
#- include: install_spark.yml

# uncomment the following line to install hive
- include: install_hive.yml

# Install mapr-metrics
- hosts: hiveserver
  sudo: yes
  roles:
    - { role: metrics-database,
        mysql_root_user: root,
        mysql_root_password: mapr,
        mysql_host: "{{ groups['hiveserver'] | first }}" }

- hosts: webserver:jobtracker
  sudo: yes
  roles:
    - { role: mapr-metrics,
        cluster_name: '{{mapr_cluster_name}}',
        metrics_user: maprmetrics,
        metrics_password: mapr,
        metrics_host: "{{ groups['hiveserver'] | first }}" }

# For convenience, print the URL for the MCS.
- hosts: webserver
  sudo: yes
  gather_facts: no
  tasks:
    - name: get ec2 facts
      ec2_facts:
      register: ec2_facts

    - name: print webserver URLs
      debug: msg="webserver = https://{{ec2_facts.ansible_facts.ansible_ec2_public_hostname}}:8443"
      when: '"ansible_ec2_public_hostname" in ec2_facts.ansible_facts.keys()'

# if this is a virtualbox node, print out the IP for eth1, which is a host interface.
- hosts: virtualbox
  tasks:
    - name: print webserver URLs
      debug: msg="webserver=https://{{ansible_eth1.ipv4.address}}:8443"
      when: "ec2_facts is not defined"