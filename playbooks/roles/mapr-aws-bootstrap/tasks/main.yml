---
# tasks file for mapr-aws-bootstrap
  - name: install boto for ec2 module
    sudo: yes
    pip: name=boto state=present

  - name: create spot nodes for non-cluster roles
    action:
      module: ec2
      region: '{{ec2_region}}'
      zone: '{{ec2_zone}}'
      spot_price: '{{edge_node_price}}'
      key_name: '{{ec2_keypair}}'
      group: '{{ec2_security_group}}'
      image: '{{ec2_image}}'
      vpc_subnet_id: '{{vpc_subnet}}'
      assign_public_ip: true
      instance_type: '{{edge_node_type}}'
      count: '{{edge_node_count}}'
      wait: yes
      wait_timeout: '{{ec2_wait_timeout}}'
      volumes:
      - device_name: '{{root_device_path}}'
        volume_size: 24
        delete_on_termination: true
      monitoring: yes
      instance_tags:
        Name: '{{ec2_name_tag}}'
    register: external_spot
    when: edge_node_price is defined

  - name: create on-demand nodes for non-cluster roles
    action:
      module: ec2
      region: '{{ec2_region}}'
      zone: '{{ec2_zone}}'
      # spot_price: '{{edge_node_price}}'
      key_name: '{{ec2_keypair}}'
      group: '{{ec2_security_group}}'
      image: '{{ec2_image}}'
      vpc_subnet_id: '{{vpc_subnet}}'
      assign_public_ip: true
      instance_type: '{{edge_node_type}}'
      count: '{{edge_node_count}}'
      wait: yes
      wait_timeout: '{{ec2_wait_timeout}}'
      volumes:
      - device_name: '{{root_device_path}}'
        volume_size: 24
        delete_on_termination: true
      monitoring: yes
      instance_tags:
        Name: '{{ec2_name_tag}}'
    register: external_ondemand
    when: edge_node_price is not defined

  - name: create some spot cluster nodes
    action:
      module: ec2
      region: '{{ec2_region}}'
      zone: '{{ec2_zone}}'
      spot_price: '{{cluster_node_price}}'
      key_name: '{{ec2_keypair}}'
      group: '{{ec2_security_group}}'
      image: '{{ec2_image}}' # This is a CentOS 6 image
      vpc_subnet_id: '{{vpc_subnet}}'
      assign_public_ip: true
      instance_type: '{{cluster_node_type}}'
      count: '{{cluster_node_count}}'
      wait: yes
      wait_timeout: '{{ec2_wait_timeout}}'
      volumes:
      - device_name: '{{root_device_path}}'
        # confusingly, the root device is referred to as /dev/sda in AWS console, but
        # the device in linux is /dev/xvde. If you want to change the size of the underlying
        # volume as we are here, the device_name above MUST be /dev/sda. Unsure whether this
        # changes with different AMIs. Additional volumes do not seem to have this problem.
        volume_size: 24
        delete_on_termination: true
      - device_name: /dev/xvdf
        volume_size: 128
        delete_on_termination: true
      - device_name: /dev/xvdg
        volume_size: 128
        delete_on_termination: true
      monitoring: yes
      instance_tags:
        Name: '{{ec2_name_tag}}'
    register: ec2_cluster_spot
    when: cluster_node_price is defined

  - name: create some on-demand cluster nodes
    action:
      module: ec2
      region: '{{ec2_region}}'
      zone: '{{ec2_zone}}'
      key_name: '{{ec2_keypair}}'
      group: '{{ec2_security_group}}'
      image: '{{ec2_image}}' # This is a CentOS 6 image
      vpc_subnet_id: '{{vpc_subnet}}'
      assign_public_ip: true
      instance_type: '{{cluster_node_type}}'
      count: '{{cluster_node_count}}'
      wait: yes
      wait_timeout: '{{ec2_wait_timeout}}'
      volumes:
      - device_name: '{{root_device_path}}'
        # confusingly, the root device is referred to as /dev/sda in AWS console, but
        # the device in linux is /dev/xvde. If you want to change the size of the underlying
        # volume as we are here, the device_name above MUST be /dev/sda. Unsure whether this
        # changes with different AMIs. Additional volumes do not seem to have this problem.
        volume_size: 24
        delete_on_termination: true
      - device_name: /dev/xvdf
        volume_size: 128
        delete_on_termination: true
      - device_name: /dev/xvdg
        volume_size: 128
        delete_on_termination: true
      monitoring: yes
      instance_tags:
        Name: '{{ec2_name_tag}}'
    register: ec2_cluster_ondemand
    when: cluster_node_price is not defined

  - name: write an inventory file containing the just-created cluster (development)
    template: src=cluster.hosts.dev.j2 dest='./cluster.hosts' mode=0644 backup=yes
    when: cluster_node_count == 3

  - name: write an inventory file containing the just-created cluster (small-production)
    template: src=cluster.hosts.smallprod.j2 dest='./cluster.hosts' mode=0644 backup=yes
    when: cluster_node_count == 6

  - name: wait for the hosts to be available via ssh before moving on
    local_action: wait_for host='{{item.private_ip}}' port=22 delay=30 timeout=300 state=started
    with_items: ec2_cluster_spot.instances
    when: ec2_cluster_spot.instances is defined

  - name: wait for the hosts to be available via ssh before moving on
    local_action: wait_for host='{{item.private_ip}}' port=22 delay=30 timeout=300 state=started
    with_items: ec2_cluster_ondemand.instances
    when: ec2_cluster_ondemand.instances is defined

  - name: add the new hosts to inventory
    add_host: name={{item.private_ip}} groups=instances ansible_ssh_user='{{ssh_user}}'
    with_items: ec2_cluster_ondemand.instances
    when: ec2_cluster_ondemand.instances is defined

  - name: add the new hosts to inventory
    add_host: name={{item.private_ip}} groups=instances ansible_ssh_user='{{ssh_user}}'
    with_items: ec2_cluster_spot.instances
    when: ec2_cluster_spot.instances is defined

  - name: tag the instances setting user to the keypair name
    local_action: ec2_tag resource={{ item.id }} region='{{ec2_region}}' state=present
    with_items: ec2_cluster_spot.instances
    args:
      tags:
        user: '{{ec2_keypair}}'
    when: ec2_cluster_spot.instances is defined

  - name: tag the instances setting user to the keypair name
    local_action: ec2_tag resource={{ item.id }} region='{{ec2_region}}' state=present
    with_items: ec2_cluster_ondemand.instances
    args:
      tags:
        user: '{{ec2_keypair}}'
    when: ec2_cluster_ondemand.instances is defined

  - name: tag the edge nodes setting user to the keypair name
    local_action: ec2_tag resource={{ item.id }} region='{{ec2_region}}' state=present
    with_items: external_ondemand.instances
    args:
      tags:
        user: '{{ec2_keypair}}'
    when: external_ondemand.instances is defined

  - name: tag the edge nodes setting user to the keypair name
    local_action: ec2_tag resource={{ item.id }} region='{{ec2_region}}' state=present
    with_items: external_spot.instances
    args:
      tags:
        user: '{{ec2_keypair}}'
    when: external_spot.instances is defined
